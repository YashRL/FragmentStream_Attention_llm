{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10462093,"sourceType":"datasetVersion","datasetId":6477171},{"sourceId":229530,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":195728,"modelId":217627}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n\n# Inspired by Andrej Karpathy’s philosophy of \"beautiful numbers\", I have chosen numbers like 128 (because 128 = 2^7, and powers of 2 are just inherently satisfying).  \n# It’s not just math— it’s art. And science. And maybe a bit of superstition.  \n\n# hyperparameters\nbatch_size = 32\ndropout = 0.125 # (1/8) or 2^-3 \nlearning_rate = 3e-4\nmax_iters = 500000  # don't worry\nblock_size = 512\neval_interval = 50  # Because I want results more often\neval_iters = 200\nn_embd = 128\nn_head = 8\nn_layer = 8  # More layers = more depth, more brilliance.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\ndef prepare_separate_datasets(train_path, test_path, tokenizer):\n    \"\"\"\n    Load and prepare separate training and testing datasets.\n    \n    Args:\n        train_path (str): Path to training data JSON file\n        test_path (str): Path to testing data JSON file\n        tokenizer: Trained tokenizer instance\n    \n    Returns:\n        tuple: (train_data, test_data) as torch tensors\n    \"\"\"\n    # Helper function to process single file\n    def process_file(file_path):\n        with open(file_path, 'r', encoding='utf-8') as f:\n            qa_pairs = json.load(f)\n        \n        encoded_data = []\n        for pair in qa_pairs:\n            question = f\"<|Q|>{pair['Question']}\"\n            answer = f\"<|A|>{pair['Answer']}<|END|>\"\n            combined = question + answer\n            encoded = tokenizer.encode(combined).ids\n            encoded_data.extend(encoded)\n        \n        return torch.tensor(encoded_data, dtype=torch.long)\n\n    # Process both datasets\n    train_data = process_file(train_path)\n    test_data = process_file(test_path)\n    \n    return train_data, test_data\n\n\ndef create_tokenizer(train_path):\n    \"\"\"\n    Initialize and train tokenizer using only the training data\n    \n    Args:\n        train_path (str): Path to training data JSON file\n    \n    Returns:\n        tokenizer: Trained tokenizer instance\n    \"\"\"\n    tokenizer = Tokenizer(BPE())\n    tokenizer.pre_tokenizer = Whitespace()\n\n    # Load and prepare training data\n    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n        qa_pairs = json.load(f)\n        texts = []\n        for pair in qa_pairs:\n            question = f\"<|Q|>{pair['Question']}\"\n            answer = f\"<|A|>{pair['Answer']}\"\n            texts.append(question + answer)\n\n    # Train tokenizer\n    trainer = BpeTrainer(\n        special_tokens=[\"<|Q|>\", \"<|A|>\", \"<PAD>\", \"<UNK>\", \"<|END|>\"],\n        vocab_size=8000\n    )\n    tokenizer.train_from_iterator(texts, trainer=trainer)\n    return tokenizer\n\n# Data loading and preprocessing\ndef prepare_data(data_path, tokenizer):\n    with open(data_path, 'r', encoding='utf-8') as f:\n        qa_pairs = json.load(f)\n\n    encoded_data = []\n    for pair in qa_pairs:\n        question = f\"<|Q|>{pair['Question']}\"\n        answer = f\"<|A|>{pair['Answer']}<|END|>\"\n        combined = question + answer\n        encoded = tokenizer.encode(combined).ids\n        encoded_data.extend(encoded)\n\n    data = torch.tensor(encoded_data, dtype=torch.long)\n    n = int(0.7 * len(data))\n    return data[:n], data[n:]  # train, val split\n\ndef get_batch(data, batch_size, block_size):\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n@torch.no_grad()\ndef estimate_loss(model, train_data, test_data):\n    out = {}\n    model.eval()\n    for split, data in [('train', train_data), ('test', test_data)]:  # Changed 'val' to 'test'\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(data, batch_size, block_size)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\n#Now here is my Magnum Opus: \"FragmentStream_Attention\"\n\n\n# NOTE: This is my humble attempt at flash attention from scratch, because my GPU isn't blessed with Aphere or Hopper Architecture\n#So, use it on your own risk it may make everything looks so fast but it maybe put your GPU on fire\n#Best of Luck, and may your VRAM survive!  -Yash Rawal\n\n\n# # Traditional Attention (simplified)\n# B, T, C = x.shape  # B=batch size, T=sequence length, C=dimensions\n# q = self.query(x)  # (B, T, C)\n# k = self.key(x)    # (B, T, C)\n\n\n# # Store ALL attention scores at once!\n# attention_scores = q @ k.transpose(-2, -1)  # (B, T, T) - This is huge!\n# attention = softmax(attention_scores) @ v    # More memory usage\n\n\n# Now what I did is simply divided the process into batches\n\n\n# # Our FragmentStream_Attention implementation (simplified)\n\n# fragment_size = 128  # Process 128 tokens at a time\n# for i in range(0, T, fragment_size):  # Process queries in fragments\n#     q_fragment = q[:, i:i+fragment_size]  # Take small group of queries\n#     for j in range(0, T, fragment_size):  # Process keys/values in fragments\n#         k_fragment = k[:, j:j+fragment_size]  # Take small group of keys\n#         v_fragment = v[:, j:j+fragment_size]  # And corresponding values        \n#         # Compare only these small fragments\n#         scores = q_fragment @ k_fragment.transpose(-2, -1)\n#         # Process and accumulate results\n\n\n#example:\n# [Full Matrix in Memory]                              # [fragment 1]   [Clean Up]   [fragment 2]   [Clean Up]\n# X X X X X X X X X X                                  # X X X       ➜           X X X     ➜ \n# X X X X X X X X X X            =========>>>          # X X X       ➜           X X X     ➜ \n# X X X X X X X X X X                                  # X X X       ➜           X X X     ➜ \n# X X X X X X X X X X                                  # X X X       ➜           X X X     ➜ \n\n\n# Yes It may sound funny but it make signifact changes\n\n\nclass FragmentStream_Attention(nn.Module):\n    def __init__(self, head_size, block_size, dropout):\n        super().__init__()\n        self.head_size = head_size\n        self.fragment_size = 128  # Adjust based on your GPU memory\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        \n    def forward(self, q, k, v):\n        B, T, C = q.shape\n        \n        # Initialize output tensor\n        out = torch.zeros_like(v)\n        \n        # Process attention in fragments to save memory\n        for i in range(0, T, self.fragment_size):\n            j_start = i\n            j_end = min(T, i + self.fragment_size)\n            \n            # Current fragment of queries\n            q_fragment = q[:, i:j_end]\n            \n            # Calculate attention scores for this fragment\n            attn_weights = torch.zeros(B, j_end-i, T, device=q.device)\n            \n            for j in range(0, T, self.fragment_size):\n                k_fragment = k[:, j:min(T, j + self.fragment_size)]\n                \n                # Compute attention scores for this block\n                scores = (q_fragment @ k_fragment.transpose(-2, -1)) * (C ** -0.5)\n                \n                # Apply causal mask\n                scores = scores.masked_fill(\n                    self.tril[i:j_end, j:min(T, j + self.fragment_size)] == 0, \n                    float('-inf')\n                )\n                \n                attn_weights[:, :, j:min(T, j + self.fragment_size)] = scores\n            \n            # Softmax over the entire sequence length\n            attn_weights = F.softmax(attn_weights, dim=-1)\n            attn_weights = self.dropout(attn_weights)\n            \n            # Compute weighted sum of values in fragments\n            for j in range(0, T, self.fragment_size):\n                v_fragment = v[:, j:min(T, j + self.fragment_size)]\n                out[:, i:j_end] += attn_weights[:, :, j:min(T, j + self.fragment_size)] @ v_fragment\n                \n        return out\n\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.attention = FragmentStream_Attention(head_size, block_size, dropout) # Using our custom fragment-based attention\n        \n        # TODO!!<<<=== If our hardware and PyTorch version will support the flash attention then it will use flash attention else it will use our own logic of chuncked attention which replicates the basic flash attention\n        \n    def forward(self, x):\n        B, T, C = x.shape  # B: batch size, T: sequence length, C: number of channels/features\n        k = self.key(x)\n        q = self.query(x)\n        v = self.value(x)\n        out = self.attention(q, k, v) # Get the attention output using the custom attention mechanism\n        return out\n\n        \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, max_heads, head_size, head_selector=None):\n        super().__init__()\n        self.max_heads = max_heads\n        self.head_size = head_size\n        self.head_selector = head_selector  # A function that decides how many heads to use based on input sequence length\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(max_heads)])  # A list of attention heads\n        self.proj = nn.Linear(head_size * max_heads, n_embd)   # Projection layer to combine outputs of multiple heads\n        self.dropout = nn.Dropout(dropout)   # Dropout for regularization\n\n\n    def forward(self, x):\n        B, T, C = x.shape\n        \n        # Determine how many heads to use based on sequence length (adaptive heads)\n        num_heads_to_use = self.head_selector(T) if self.head_selector else self.max_heads\n        \n        # Select the relevant heads\n        selected_heads = self.heads[:num_heads_to_use]\n        \n        # Process selected heads in parallel\n        head_outputs = [h(x) for h in selected_heads]\n\n        # Concatenate the outputs of the selected heads and project them to the original embedding size\n        out = torch.cat(head_outputs, dim=-1)\n        out = self.dropout(self.proj(out)) # Apply dropout for regularization\n        return out\n\n    def head_selector(sequence_length):\n        \"\"\"\n        A simple function to select the number of heads based on sequence length.\n        Fewer heads for smaller sequences, more heads for larger sequences.\n        \"\"\"\n        if sequence_length > 512:\n            return 12  # Use 12 heads for longer sequences\n        elif sequence_length > 256:\n            return 8   # Use 8 heads for medium sequences\n        else:\n            return 4   # Use 4 heads for shorter sequences\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, activation_function='leaky_relu', dropout=0.1):\n        super().__init__()\n        self.activation_function = activation_function\n        self.linear1 = nn.Linear(n_embd, 4 * n_embd)\n        self.linear2 = nn.Linear(4 * n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        \n        #Look Here <<<<===== we are utlizing PyTorch's Dynamic Computation Graphs functionality to switch between different activation fucntions for experimentations\n        if self.activation_function == 'relu':\n            x = F.relu(x)\n        elif self.activation_function == 'gelu':\n            x = F.gelu(x)\n        elif self.activation_function == 'silu':\n            x = F.silu(x)\n        elif self.activation_function == 'leaky_relu':  # Added Leaky ReLU option\n            x = F.leaky_relu(x, negative_slope=0.01)  # Uses a fixed negative slope of 0.01 to prevent dead neurons issue\n        else:\n            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n        \n        x = self.linear2(x)\n        x = self.dropout(x)\n        return x\n\n\n##In case if you are confused WTF is going on the bellow is simple implementation of the Feed Forward algorithm\n\n# class FeedForward(nn.Module):\n#     def __init__(self, n_embd):\n#         super().__init__()\n#         self.net = nn.Sequential(\n#             nn.Linear(n_embd, 4 * n_embd),\n#             nn.ReLU(),\n#             nn.Linear(4 * n_embd, n_embd),\n#             nn.Dropout(dropout),\n#         )\n\n#     def forward(self, x):\n#         return self.net(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    #This is Pre-LayerNorm\n    def forward(self, x):\n        x = self.sa(self.ln1(x)) + x\n        x = self.ffwd(self.ln2(x)) + x\n        return x\n\n\n    ##This is Post-LayerNorm \n    # def forward(self, x):\n    #     x = x self.sa(self.ln1(x))\n    #     x = x + self.ffwd(self.ln2(x))\n    #     return x\n\n    ##WHY!!: Pre-LayerNorm improves training stability, especially for deep models, and can mitigate gradient vanishing/explosion issues.\n\n\nclass LanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens, tokenizer, temperature=0.8, stop_token=\"<|Q|>\"):\n        \"\"\"\n        Generate text with improved sampling and better control.\n        \n        Args:\n            idx: Input token indices (B, T)\n            max_new_tokens: Maximum number of tokens to generate\n            tokenizer: Tokenizer instance\n            temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n            stop_token: Token to stop generation\n            \n        Returns:\n            torch.Tensor: Generated token indices\n        \"\"\"\n        # Store original shape\n        B, T = idx.shape\n        \n        for _ in range(max_new_tokens):\n            # Crop sequence if it exceeds block_size\n            idx_cond = idx[:, -block_size:]\n            \n            # Get model predictions\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]  # Focus on last token prediction\n            \n            # Apply temperature\n            if temperature == 0.0:\n                # Greedy sampling\n                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n            else:\n                # Apply temperature to logits\n                logits = logits / temperature\n                # Apply softmax to get probabilities\n                probs = F.softmax(logits, dim=-1)\n                # Sample from the distribution\n                idx_next = torch.multinomial(probs, num_samples=1)\n            \n            # Append next token\n            idx = torch.cat((idx, idx_next), dim=1)\n            \n            # Check for stop token or end of text\n            decoded = tokenizer.decode(idx[0].tolist())\n            if stop_token in decoded or len(idx[0]) >= block_size * 2:\n                break\n        \n        return idx\n\n        ##I know the temprature logic looks ugly but it works NICE! and we will find something Aesthetic later\n    \ndef count_parameters(model):\n    \"\"\"Count number of trainable parameters in the model\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef print_model_info(model, train_data, test_data):\n    \"\"\"Print model architecture and training setup information\"\"\"\n    n_params = count_parameters(model)\n    \n    print(\"=\" * 50)\n    print(\"Model Configuration:\")\n    print(\"=\" * 50)\n    print(f\"Number of trainable parameters: {n_params:,}\")\n    print(f\"Number of layers: {n_layer}\")\n    print(f\"Number of heads: {n_head}\")\n    print(f\"Embedding dimension: {n_embd}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Block size: {block_size}\")\n    print(f\"Learning rate: {learning_rate}\")\n    print(f\"Dropout: {dropout}\")\n    print(\"\\nDataset Information:\")\n    print(f\"Training samples: {len(train_data):,}\")\n    print(f\"Test samples: {len(test_data):,}\")\n    print(f\"Device: {device}\")\n    print(\"=\" * 50)\n\ndef train_model(train_path, test_path, resume_checkpoint=None):\n    # Initialize tokenizer using training data only\n    print(\"Initializing and training tokenizer...\")\n    tokenizer = create_tokenizer(train_path)\n    \n    # Prepare separate datasets\n    print(\"Preparing datasets...\")\n    train_data, test_data = prepare_separate_datasets(train_path, test_path, tokenizer)\n\n    # Initialize model\n    print(\"Initializing model...\")\n    model = LanguageModel(tokenizer.get_vocab_size())\n    model = model.to(device)\n\n    # Print model information\n    print_model_info(model, train_data, test_data)\n\n    # Load checkpoint if provided\n    start_iter = 0\n    if resume_checkpoint and os.path.exists(resume_checkpoint):\n        model.load_state_dict(torch.load(resume_checkpoint, weights_only=True))\n        start_iter = int(resume_checkpoint.split('_')[-1].split('.')[0])\n        print(f\"Resuming from checkpoint: {resume_checkpoint}\")\n\n    # Optimizer and scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    scheduler = CosineAnnealingLR(optimizer, T_max=max_iters)\n\n    # Initialize lists to store loss history\n    train_losses = []\n    test_losses = []\n    \n    print(\"Starting training...\")\n    print(\"=\" * 50)\n\n    # Training loop\n    best_test_loss = float('inf')\n    for iter in range(start_iter, max_iters):\n        if iter % eval_interval == 0:\n            losses = estimate_loss(model, train_data, test_data)\n            train_loss = losses['train']\n            test_loss = losses['test']\n            \n            train_losses.append(train_loss)\n            test_losses.append(test_loss)\n            \n            print(f\"Step {iter}: train loss {train_loss:.4f}, test loss {test_loss:.4f}\")\n            \n            # Save best model\n            if test_loss < best_test_loss:\n                best_test_loss = test_loss\n                torch.save(model.state_dict(), 'best_model_.yash')\n                print(f\"New best model saved!\")\n\n        xb, yb = get_batch(train_data, batch_size, block_size)\n        logits, loss = model(xb, yb)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        if iter % 1000 == 0:\n            checkpoint_path = f'checkpoint_step_{iter}.yash'\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f\"Checkpoint saved at step {iter}\")\n            \n            # Print current learning rate\n            current_lr = optimizer.param_groups[0]['lr']\n            print(f\"Current learning rate: {current_lr:.6f}\")\n\n    print(\"\\nTraining completed!\")\n    print(\"=\" * 50)\n    print(f\"Best test loss: {best_test_loss:.4f}\")\n    print(f\"Final learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n    return model, tokenizer\n\n# Example usage\nif __name__ == \"__main__\":\n    train_path = \"/kaggle/input/mini-clinical-dataset/medical_train.json\"\n    test_path = \"/kaggle/input/mini-clinical-dataset/medical.json\"\n    resume_checkpoint = \"/kaggle/input/mini_healthcare_v1/pytorch/default/1/checkpoint_step_9000.yash\"  # or path to checkpoint\n    model, tokenizer = train_model(train_path, test_path, resume_checkpoint)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:21:40.729852Z","iopub.execute_input":"2025-01-14T08:21:40.730167Z","iopub.status.idle":"2025-01-14T10:39:02.710917Z","shell.execute_reply.started":"2025-01-14T08:21:40.730142Z","shell.execute_reply":"2025-01-14T10:39:02.709605Z"}},"outputs":[{"name":"stdout","text":"Initializing and training tokenizer...\nPreparing datasets...\nInitializing model...\n==================================================\nModel Configuration:\n==================================================\nNumber of trainable parameters: 3,704,896\nNumber of layers: 8\nNumber of heads: 8\nEmbedding dimension: 128\nBatch size: 32\nBlock size: 512\nLearning rate: 0.0003\nDropout: 0.125\n\nDataset Information:\nTraining samples: 26,075,321\nTest samples: 1,368,819\nDevice: cuda\n==================================================\nResuming from checkpoint: /kaggle/input/mini_healthcare_v1/pytorch/default/1/checkpoint_step_9000.yash\nStarting training...\n==================================================\nStep 9000: train loss 3.8662, test loss 3.8918\nNew best model saved!\nCheckpoint saved at step 9000\nCurrent learning rate: 0.000300\nStep 9050: train loss 3.8590, test loss 3.8832\nNew best model saved!\nStep 9100: train loss 3.8552, test loss 3.8847\nStep 9150: train loss 3.8635, test loss 3.8773\nNew best model saved!\nStep 9200: train loss 3.8504, test loss 3.8955\nStep 9250: train loss 3.8472, test loss 3.8831\nStep 9300: train loss 3.8525, test loss 3.8817\nStep 9350: train loss 3.8473, test loss 3.8796\nStep 9400: train loss 3.8453, test loss 3.8728\nNew best model saved!\nStep 9450: train loss 3.8383, test loss 3.8706\nNew best model saved!\nStep 9500: train loss 3.8412, test loss 3.8708\nStep 9550: train loss 3.8498, test loss 3.8773\nStep 9600: train loss 3.8436, test loss 3.8634\nNew best model saved!\nStep 9650: train loss 3.8363, test loss 3.8689\nStep 9700: train loss 3.8323, test loss 3.8524\nNew best model saved!\nStep 9750: train loss 3.8303, test loss 3.8648\nStep 9800: train loss 3.8364, test loss 3.8592\nStep 9850: train loss 3.8325, test loss 3.8570\nStep 9900: train loss 3.8343, test loss 3.8670\nStep 9950: train loss 3.8127, test loss 3.8486\nNew best model saved!\nStep 10000: train loss 3.8309, test loss 3.8556\nCheckpoint saved at step 10000\nCurrent learning rate: 0.000300\nStep 10050: train loss 3.8174, test loss 3.8484\nNew best model saved!\nStep 10100: train loss 3.8053, test loss 3.8604\nStep 10150: train loss 3.8079, test loss 3.8461\nNew best model saved!\nStep 10200: train loss 3.8024, test loss 3.8491\nStep 10250: train loss 3.8074, test loss 3.8475\nStep 10300: train loss 3.8085, test loss 3.8467\nStep 10350: train loss 3.8134, test loss 3.8401\nNew best model saved!\nStep 10400: train loss 3.8092, test loss 3.8426\nStep 10450: train loss 3.8030, test loss 3.8334\nNew best model saved!\nStep 10500: train loss 3.8003, test loss 3.8392\nStep 10550: train loss 3.8015, test loss 3.8343\nStep 10600: train loss 3.8057, test loss 3.8432\nStep 10650: train loss 3.8038, test loss 3.8367\nStep 10700: train loss 3.7991, test loss 3.8314\nNew best model saved!\nStep 10750: train loss 3.7997, test loss 3.8308\nNew best model saved!\nStep 10800: train loss 3.8047, test loss 3.8291\nNew best model saved!\nStep 10850: train loss 3.7821, test loss 3.8209\nNew best model saved!\nStep 10900: train loss 3.7866, test loss 3.8211\nStep 10950: train loss 3.7997, test loss 3.8242\nStep 11000: train loss 3.7900, test loss 3.8213\nCheckpoint saved at step 11000\nCurrent learning rate: 0.000300\nStep 11050: train loss 3.7698, test loss 3.8237\nStep 11100: train loss 3.7983, test loss 3.8252\nStep 11150: train loss 3.7843, test loss 3.8206\nNew best model saved!\nStep 11200: train loss 3.7824, test loss 3.8258\nStep 11250: train loss 3.7817, test loss 3.8224\nStep 11300: train loss 3.7734, test loss 3.8067\nNew best model saved!\nStep 11350: train loss 3.7647, test loss 3.8098\nStep 11400: train loss 3.7702, test loss 3.8035\nNew best model saved!\nStep 11450: train loss 3.7817, test loss 3.8106\nStep 11500: train loss 3.7778, test loss 3.8037\nStep 11550: train loss 3.7667, test loss 3.8009\nNew best model saved!\nStep 11600: train loss 3.7659, test loss 3.8056\nStep 11650: train loss 3.7772, test loss 3.8030\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36m<cell line: 536>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/mini-clinical-dataset/medical.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0mresume_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/mini_healthcare_v1/pytorch/default/1/checkpoint_step_9000.yash\"\u001b[0m  \u001b[0;31m# or path to checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_path, test_path, resume_checkpoint)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;31m#This is Pre-LayerNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Process selected heads in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mhead_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_heads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Process selected heads in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mhead_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_heads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2b2ce630e0cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0;31m# Apply causal mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 scores = scores.masked_fill(\n\u001b[0m\u001b[1;32m    212\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# Initialize and train tokenizer\ntokenizer = Tokenizer(BPE())\ntokenizer.pre_tokenizer = Whitespace()\n\n# Load your training data\nwith open(\"/kaggle/input/mini-clinical-dataset/medical_train.json\", \"r\", encoding=\"utf-8\") as f:\n    qa_pairs = json.load(f)\n    texts = []\n    for pair in qa_pairs:\n        question = f\"<|Q|>{pair['Question']}\"\n        answer = f\"<|A|>{pair['Answer']}\"\n        texts.append(question + answer)\n\n# Train the tokenizer\ntrainer = BpeTrainer(\n    special_tokens=[\"<|Q|>\", \"<|A|>\", \"<PAD>\", \"<UNK>\", \"<|END|>\"],\n    vocab_size=8000 #make sure it matches what you used in training\n)\ntokenizer.train_from_iterator(texts, trainer=trainer)\n\n# Save the tokenizer\ntokenizer.save(\"tokenizer.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T08:19:34.635402Z","iopub.execute_input":"2025-01-14T08:19:34.635674Z","iopub.status.idle":"2025-01-14T08:19:44.563384Z","shell.execute_reply.started":"2025-01-14T08:19:34.635653Z","shell.execute_reply":"2025-01-14T08:19:44.562389Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from tokenizers import Tokenizer\nimport torch\nimport torch.nn as nn\n\n# 1. Load the tokenizer and model with proper error handling\ntry:\n    tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n    checkpoint_path = '/kaggle/working/best_model_.yash'\n    model = LanguageModel(vocab_size=tokenizer.get_vocab_size()).to(device)\n    model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n    model.eval()\nexcept Exception as e:\n    print(f\"Error loading model or tokenizer: {e}\")\n    raise\n\n# 2. Improved generate_answer function with temperature control\ndef generate_answer(question, max_tokens=200, temperature=0.6):\n    \"\"\"\n    Generate an answer for a given question with temperature control.\n    # 0.2-0.4: More focused, deterministic responses\n    # 0.5-0.7: Balanced responses\n    # 0.7-0.9: More creative, diverse responses\n    # 0.0: Completely deterministic (greedy)\n    \n    Args:\n        question (str): The input question\n        max_tokens (int): Maximum number of tokens to generate\n        temperature (float): Sampling temperature (0.0 = greedy, 1.0 = more random)\n    \n    Returns:\n        str: Generated answer or error message\n    \"\"\"\n    try:\n        # Format and encode the input\n        formatted_input = f\"<|Q|>{question}<|A|>\"\n        encoded_input = tokenizer.encode(formatted_input)\n        input_ids = torch.tensor([encoded_input.ids], dtype=torch.long, device=device)\n        \n        # Generate response\n        with torch.no_grad():\n            generated_ids = model.generate(\n                input_ids,\n                max_new_tokens=max_tokens,\n                tokenizer=tokenizer,\n                temperature=temperature,\n                stop_token=\"<|Q|>\"\n            )\n        \n        # Decode and clean the generated text\n        generated_text = tokenizer.decode(generated_ids[0].tolist())\n        \n        # Extract answer with improved handling\n        if \"<|A|>\" in generated_text:\n            parts = generated_text.split(\"<|A|>\")\n            if len(parts) > 1:\n                # Get everything after <|A|> but before <|Q|> or <|END|>\n                answer = parts[1].split(\"<|Q|>\")[0].split(\"<|END|>\")[0].strip()\n                \n                # Basic cleaning: remove repeated spaces and fix common issues\n                answer = ' '.join(answer.split())  # Remove multiple spaces\n                answer = answer.replace(\" .\", \".\")  # Fix spacing around punctuation\n                answer = answer.replace(\" ,\", \",\")\n                \n                return answer\n            else:\n                return \"Error: Could not extract answer properly\"\n        else:\n            return generated_text.strip()\n            \n    except Exception as e:\n        print(f\"Error during generation: {e}\")\n        return f\"Error generating response: {str(e)}\"\n\n# 3. Enhanced test generation function with temperature control\ndef test_generation(question, temperature=0.6):\n    \"\"\"\n    Test the model's generation with different temperature settings.\n    \n    Args:\n        question (str): The input question\n        temperature (float): Sampling temperature (0.0 = greedy, 1.0 = more random)\n    \"\"\"\n    print(f\"Generating answer (temperature={temperature})...\")\n    answer = generate_answer(question, temperature=temperature)\n    print(\"\\nResults:\")\n    print(f\"A: {answer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:39:27.329378Z","iopub.execute_input":"2025-01-14T10:39:27.329668Z","iopub.status.idle":"2025-01-14T10:39:27.644669Z","shell.execute_reply.started":"2025-01-14T10:39:27.329645Z","shell.execute_reply":"2025-01-14T10:39:27.644018Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"question = \"Hello\"\ntest_generation(question, temperature=0.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:39:32.185701Z","iopub.execute_input":"2025-01-14T10:39:32.185998Z","iopub.status.idle":"2025-01-14T10:39:40.825657Z","shell.execute_reply.started":"2025-01-14T10:39:32.185974Z","shell.execute_reply":"2025-01-14T10:39:40.824804Z"}},"outputs":[{"name":"stdout","text":"Generating answer (temperature=0.0)...\n\nResults:\nA: Hello Hi , I am Chat Doctor , I am Chat Doctor , infectious diseases specialist , answering your query . I will try to help you as much as much as I can . I can understand your concern . You should consult your doctor and get done clinical examination of respiratory system and PFT ( Pulmonary Function Test ). If needed , you should go for fine needle aspiration cytology or biopsy of respiratory system . If needed go for fine needle aspiration cytology or biopsy of respiratory system . If needed go for fine needle aspiration cytology or biopsy of respiratory system . If needed go for fine needle aspiration cytology or biopsy of respiratory system . If needed go for fine needle aspiration cytology or biopsy of respiratory system . Hope I have answered your question , if you have doubt then I will be happy to answer . Thanks for using Chat Doctor . Wish you a very good health . I have a lump on my left side of my left arm and it is not painful . It is not painful . It is not painful . It is not painful to\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"question = \"I am having pain in my head\"\ntest_generation(question, temperature=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:39:40.826853Z","iopub.execute_input":"2025-01-14T10:39:40.827165Z","iopub.status.idle":"2025-01-14T10:39:49.912268Z","shell.execute_reply.started":"2025-01-14T10:39:40.827134Z","shell.execute_reply":"2025-01-14T10:39:49.911498Z"}},"outputs":[{"name":"stdout","text":"Generating answer (temperature=0.5)...\n\nResults:\nA: I am having pain in my head Hello , As per your query you have symptoms of wisdom teeth , I would suggest you to consult a dentist for proper evaluation and treatment . I would suggest you to consult a dentist for proper examination and get it done for proper diagnosis and treatment . I hope that you have found something helpful . Take care . Hi , I have been suffering from a white blood count , and a fever . I have been on the medication for 6 months now and my doctor gave me an antibiotic for a few days and my stomach was not relieved . I went to the doctor and he prescribed me an antibiotic prescribed me to continue the antibiotics . I am also taking C ef lex for a couple of days . I am taking the medicine for the pain . I have taken a lot of antibiotics and have never been taking antibiotics if I have a very severe stomach pain . I am now getting a pain in my stomach . I am also having a lot of pain in my stomach . I have a very well life long time , but\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"question = \"I wake in the night, usually about 2-3 hours after going to sleep, with both feet and legs to mid calf feeling like they are on fire. slight red discolorization, minor swelling. This is very painful but after getting up, I can walk it off in about 30 minutes.\"\ntest_generation(question, temperature=0.7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:39:49.913554Z","iopub.execute_input":"2025-01-14T10:39:49.913798Z","iopub.status.idle":"2025-01-14T10:40:01.530005Z","shell.execute_reply.started":"2025-01-14T10:39:49.913778Z","shell.execute_reply":"2025-01-14T10:40:01.529236Z"}},"outputs":[{"name":"stdout","text":"Generating answer (temperature=0.7)...\n\nResults:\nA: I wake in the night , usually about 2 - 3 hours after going to sleep , with both feet and legs to mid calf feeling like they are on fire . slight red dis color ization , minor swelling . This is very painful but after getting up , I can walk it off in about 30 minutes . Hi . Welcome to Chat Doctor . I have gone through your query and understand your concern . As per your complaint , it seems that you have pain in your lower back [ you have backache , which is not relieved from your posture while lying position . I would advise consulting ER your doctor for proper examination and examination . Doctor may prescribe some analgesic like diclofenac , tramadol , decongestants . I do hope my answer will help you . Take care . hi doctor , my name is Ra j us and thank you for your time to clarify the phone and i was able to work with my dad s . I have a problem with me , but i am concerned about myself i am on the medicine and my husband . is not getting pregnant . And i can get pregnant until i am worried about my last 6 years old and i have been told that i have not conceived after my menstrual cycle . Hi , Thanks for posting your query . Based on the facts of your query , your query , You / have posted it\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}